{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rs2FxEOoCNms",
        "colab_type": "code",
        "outputId": "79ffd06c-3f54-4e0f-9464-926d535823a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        }
      },
      "source": [
        "!pip install transformers\n",
        "%tensorflow_version 2.x"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/38/c9527aa055241c66c4d785381eaf6f80a28c224cae97daa1f8b183b5fabb/transformers-2.9.0-py3-none-any.whl (635kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 35.7MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 37.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=08f671dd64e47d21964e10b589e1bc5ebedfdb6e49dc7448f2a62660d1ac5799\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.86 tokenizers-0.7.0 transformers-2.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43IAVjB2CL8s",
        "colab_type": "code",
        "outputId": "e719aeb2-8984-47d0-fa90-87cc561b769d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from transformers import TFRobertaModel\n",
        "\n",
        "logger = tf.get_logger()\n",
        "logger.info(tf.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAGwaLi7CL9n",
        "colab_type": "text"
      },
      "source": [
        "### Parsing tfrecords + tf.data.Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6m_O2VPFCL9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextDataset(tf.data.Dataset):\n",
        "    feature = {}\n",
        "    for i in range(512):\n",
        "        feature['dim_' + str(i)] = tf.io.VarLenFeature(tf.int64)\n",
        "\n",
        "\n",
        "    def _parse_example(example_proto):\n",
        "        parsed_example_dict = tf.io.parse_single_example(example_proto, TextDataset.feature)\n",
        "        parsed_example = [tf.sparse.to_dense(parsed_example_dict['dim_'+str(i)]) for i in range(512)]\n",
        "        parsed_example = tf.transpose(tf.stack(parsed_example, axis=0), perm=[1, 0])\n",
        "        return parsed_example\n",
        "\n",
        "\n",
        "    def _construct_inputs(input_ids):\n",
        "        input_ids = tf.cast(input_ids, dtype=tf.int32)\n",
        "        num_papers = tf.shape(input_ids)[0]\n",
        "        idx_a = tf.random.uniform(minval=0, maxval=num_papers, shape=[], dtype=tf.int32)\n",
        "        input_a = tf.gather(input_ids, idx_a)\n",
        "        all_related_papers = tf.gather(input_ids, tf.where(\n",
        "            tf.logical_not(tf.reduce_all(tf.equal(input_ids, input_a), axis=-1)))[:, 0])\n",
        "\n",
        "        idx = tf.random.categorical(tf.zeros([1, num_papers-1], dtype=tf.float32), num_samples=4)[0]\n",
        "        input_b, input_c, input_d, input_e = tf.unstack(tf.gather(all_related_papers, idx),\n",
        "                                                        num=4,\n",
        "                                                        axis=0)\n",
        "        return input_a, input_b, input_c, input_d, input_e\n",
        "\n",
        "\n",
        "    def _parse_and_create_sample(example_proto):\n",
        "        input_ids = TextDataset._parse_example(example_proto)\n",
        "        sample = TextDataset._construct_inputs(input_ids)\n",
        "        positive_labels = tf.ones([1])\n",
        "        negative_labels = tf.ones([batch_size])\n",
        "        return sample, (negative_labels, positive_labels)\n",
        "    \n",
        "    def __new__(cls, tfrecords_pattern, epochs, batch_size):\n",
        "        _options = tf.data.Options()\n",
        "        _options.experimental_deterministic = False\n",
        "\n",
        "        tfrecords = tf.data.Dataset.list_files(tfrecords_pattern)\n",
        "        dataset = tfrecords.interleave(tf.data.TFRecordDataset,\n",
        "                                       cycle_length=4,\n",
        "                                       block_length=16,\n",
        "                                       num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "        dataset = dataset.with_options(_options)\n",
        "        dataset = dataset.map(cls._parse_and_create_sample,\n",
        "                              num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "        dataset = dataset.batch(batch_size, drop_remainder=True)\n",
        "        dataset = dataset.repeat(epochs)\n",
        "        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "        return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlETgD76CL9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size=4\n",
        "epochs = 2\n",
        "lr = 1e-5\n",
        "dataset = TextDataset('tfrecords/*', epochs=epochs, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5WrwDEACL9x",
        "colab_type": "code",
        "outputId": "c3141294-70c9-47e2-ef2d-977e35da039d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "def dot_product(x, y, pairwise=False, name=None):\n",
        "  if pairwise:\n",
        "    x = tf.expand_dims(x, axis=1)\n",
        "  return tf.reduce_sum(tf.multiply(x, y), axis=-1, name=name, keepdims=not pairwise)\n",
        "\n",
        "base_model = TFRobertaModel.from_pretrained('allenai/biomed_roberta_base', from_pt=True)\n",
        "\n",
        "inputs = [tf.keras.Input(shape=[512], dtype=tf.int32, name='input_{}'.format(i), batch_size=batch_size) for i in ['a', 'b', 'c', 'd', 'e']]\n",
        "outputs = [tf.reduce_mean(base_model(x)[0], axis=1) for x in inputs]\n",
        "\n",
        "ff1 = tf.keras.layers.Dense(768, activation='tanh', name='ff1')\n",
        "ff1_outputs = [ff1(x) for x in outputs]\n",
        "\n",
        "mean_related_papers = tf.reduce_mean(tf.concat(ff1_outputs[1:], axis=1), axis=1, keepdims=True)\n",
        "ff2 = tf.keras.layers.Dense(768, activation='tanh', name='ff2')\n",
        "ff2_output = ff2(mean_related_papers)\n",
        "\n",
        "negative_outputs = dot_product(ff1_outputs[0], ff1_outputs[0], pairwise=True, name='negative')\n",
        "positive_outputs = dot_product(ff2_output, ff2_output, pairwise=False, name='positive')\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=[negative_outputs, positive_outputs])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor 'Mean:0' shape=(4, 768) dtype=float32>,\n",
              " <tf.Tensor 'Mean_1:0' shape=(4, 768) dtype=float32>,\n",
              " <tf.Tensor 'Mean_2:0' shape=(4, 768) dtype=float32>,\n",
              " <tf.Tensor 'Mean_3:0' shape=(4, 768) dtype=float32>,\n",
              " <tf.Tensor 'Mean_4:0' shape=(4, 768) dtype=float32>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b02Ylv_y7Tor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def positive_loss(_, y_pred):\n",
        "  y_pred = tf.nn.sigmoid(y_pred)\n",
        "  y_true = tf.ones([batch_size, 1])\n",
        "  return tf.losses.binary_crossentropy(y_true, y_pred)\n",
        "\n",
        "def negative_loss(_, y_pred):\n",
        "  y_pred = tf.nn.softmax(y_pred)\n",
        "  y_true = tf.eye(batch_size)\n",
        "  return tf.losses.categorical_crossentropy(y_true, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxQn4k8h8VQF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_dict = {\n",
        "    'tf_op_layer_positive': positive_loss,\n",
        "    'tf_op_layer_negative': negative_loss\n",
        "}\n",
        "\n",
        "model.compile(loss=loss_dict, optimizer=tf.keras.optimizers.Adam(lr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmiLSacJB7Pk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "callbacks_list = [tf.keras.callbacks.ModelCheckpoint(model_dir + '/weights.{epoch:02d}', save_weights_only=True)]\n",
        "model.fit(dataset, epochs=epochs, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}